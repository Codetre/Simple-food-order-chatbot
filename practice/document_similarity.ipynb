{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from typing import List, DefaultDict\n",
    "\n",
    "import numpy as np\n",
    "from konlpy.tag._komoran import Komoran"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 문서 유사도\n",
    "원리: 두 문장 간 유사한 단어들이 얼마나 겹치는지 정도를 측정\n",
    "\n",
    "이 프로젝트는 QnA 챗봇을 만듦을 목표로 한다. 챗봇은 몇 가지 질문에 대한 답변을 가지고 있다.\n",
    "유저가 챗봇에게 질문을 하면 챗봇은 이 질문을 임베딩 변환해서 자기가 답변을 아는 질문 중 가장 유사한 것을\n",
    "찾아낸다. 찾아낸 가장 유사한 질문에 대한 답이 원래 유저가 요구했던 답변이라고 판단하는 것이다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## n-gram 유사도\n",
    "- n-gram의 예시(n=2, 문장 단위 문서 '아버지가 신문을 들고 방에서 나오며 나를 찾았다.'를 예로)\n",
    "1. 단어(명사) 추출: `['아버지', '신문', 방', '나']`\n",
    "2. TDM(Term-Document Matrix) 생성:\n",
    "\n",
    "| 아버지 | 신문  | 방 | 나   | 토큰        |\n",
    "|------|-----|----|-----|-----------|\n",
    "|   1  | 1   |  0  | 0   | (아버지, 신문) |\n",
    "|   0  | 1   |  1  | 0   | (신문, 방)   |\n",
    "|   0  | 0   |  1  | 1   | (방, 나)    |\n",
    "\n",
    "3. 토큰 정리: `[['아버지', '신문'], ['신문', '방'], ['방', '나']]`\n",
    "\n",
    "- n-gram 기반 유사도 계산\n",
    "> `similarity(doc1, doc2) = tf(doc1, doc2) / tokens(doc1)`\n",
    "> `tf(doc1, doc2)`는 term frequency로서 doc1과 doc2에서 동시에 나타나는 토큰의 빈도,\n",
    "> `tokens(doc1)`는 doc1 내 모든 토큰의 수"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def word_ngram(bag_of_words: List, num_gram: int) -> List:\n",
    "    \"\"\"n-gram 토큰 나열\n",
    "\n",
    "    :param bag_of_words: 문서 내 구별되는 단어의 나열.\n",
    "    :param num_gram: n-gram에서 n의 크기.\n",
    "    :return: 토큰 리스트.\n",
    "    \"\"\"\n",
    "    ngrams = [bag_of_words[i:i + num_gram] for i in range(len(bag_of_words))]\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def n_gram_similarity(doc1: List, doc2: List) -> float:\n",
    "    \"\"\"n-gram 기반 유사도 계산\n",
    "\n",
    "    :param doc1: `word_ngram`의 리턴. 이 문서가 doc2와 얼마나 유사한지 계산한다.\n",
    "    :param doc2: `word_ngram`의 리턴\n",
    "    :return: 유사도\n",
    "    \"\"\"\n",
    "    num_common_tokens = 0\n",
    "    for token in doc1:\n",
    "        if token in doc2:\n",
    "            num_common_tokens += 1\n",
    "    num_tokens_in_doc1 = len(doc1)\n",
    "\n",
    "    return num_common_tokens / num_tokens_in_doc1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "sentence1 = \"리바운드를 제압하는 자가 농구를 제압한다.\"\n",
    "sentence2 = \"화력으로 제압하는 부대가 상대를 제압한다.\"\n",
    "sentence3 = \"리바운드를 잘하는 자가 농구에 필요하다.\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "komoran = Komoran()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "nouns1 = komoran.nouns(sentence1)\n",
    "nouns2 = komoran.nouns(sentence2)\n",
    "nouns3 = komoran.nouns(sentence3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "['리바운드', '제압', '자', '농구', '제압']"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "num_gram = 2\n",
    "tokens1 = word_ngram(nouns1, num_gram)\n",
    "tokens2 = word_ngram(nouns2, num_gram)\n",
    "tokens3 = word_ngram(nouns3, num_gram)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['리바운드', '제압'], ['제압', '자'], ['자', '농구'], ['농구', '제압'], ['제압']]\n",
      "[['화력', '제압'], ['제압', '부대'], ['부대', '상대'], ['상대', '제압'], ['제압']]\n",
      "[['리바운드', '자'], ['자', '농구'], ['농구', '필요'], ['필요']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens1)\n",
    "print(tokens2)\n",
    "print(tokens3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "gram_sim12 = n_gram_similarity(tokens1, tokens2)\n",
    "gram_sim23 = n_gram_similarity(tokens2, tokens3)\n",
    "gram_sim31 = n_gram_similarity(tokens3, tokens1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'리바운드를 제압하는 자가 농구를 제압한다.' '화력으로 제압하는 부대가 상대를 제압한다.'의 유사도: 0.2\n",
      "'화력으로 제압하는 부대가 상대를 제압한다.' '리바운드를 잘하는 자가 농구에 필요하다.'의 유사도: 0.0\n",
      "'리바운드를 제압하는 자가 농구를 제압한다.' '리바운드를 잘하는 자가 농구에 필요하다.'의 유사도: 0.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"'{sentence1}' '{sentence2}'의 유사도: {gram_sim12}\")\n",
    "print(f\"'{sentence2}' '{sentence3}'의 유사도: {gram_sim23}\")\n",
    "print(f\"'{sentence1}' '{sentence3}'의 유사도: {gram_sim31}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 코사인 유사도\n",
    "두 임베딩 벡터 간 코사인 값으로 유사도를 측정.\n",
    "  - 두 벡터가 정반대: cos = -1\n",
    "  - 두 벡터가 직교: cos = 0\n",
    "  - 두 벡터가 같은 방향: cos = 1\n",
    "\n",
    "`similarity = cos = dot(v1, v2) / norm(v1) * norm(v2)`이다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "def cos_similarity(v1, v2):\n",
    "    \"\"\"코사인 유사도 계산\n",
    "\n",
    "    :param v1: 임베딩 벡터 1\n",
    "    :param v2: 임베딩 벡터 2\n",
    "    :return: v1, v2 간 유사도\n",
    "    \"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "def term_document_matrix(sentence_tokens, word_set) -> dict:\n",
    "    \"\"\"문장 내 토큰이 문서에서 몇 번 나왔는지 기록.\n",
    "\n",
    "    :param sentence_tokens: 특정 문장 내 토큰들.\n",
    "    :param word_set: sentence_tokens의 문장을 포함한 문서 내 구별되는 단어(명사) 집합.\n",
    "    :return: `word_set` 내에서 토큰별 빈도를 기록.\n",
    "    \"\"\"\n",
    "    tdm = {word: 0 for word in word_set}\n",
    "\n",
    "    for word in word_set:\n",
    "        if word in sentence_tokens:\n",
    "            tdm[word] += 1\n",
    "\n",
    "    return tdm\n",
    "\n",
    "\n",
    "def vectorize(tdm: DefaultDict):\n",
    "    \"\"\"토큰 출현 빈도로 벡터 생성.\n",
    "\n",
    "    :param tdm: `term_document_matrix`의 리턴값.\n",
    "    :return: 문서의 단어 집합에서 토큰이 몇 번 나왔는지를 나열한 리스트.\n",
    "    \"\"\"\n",
    "    vector = []\n",
    "    for freq in tdm.values():\n",
    "        vector.append(freq)\n",
    "\n",
    "    return vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "word_set = set(nouns1) | set(nouns2) | set(nouns3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# 위에서 'token'을 n-gram의 단위를 칭하는데 썼기에 대신, bag-of-words로 문장 내 단어 집합 칭한다.\n",
    "bow1 = komoran.nouns(sentence1)\n",
    "bow2 = komoran.nouns(sentence2)\n",
    "bow3 = komoran.nouns(sentence3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "token_freq1 = term_document_matrix(bow1, word_set)\n",
    "token_freq2 = term_document_matrix(bow2, word_set)\n",
    "token_freq3 = term_document_matrix(bow3, word_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "vec1 = vectorize(token_freq1)\n",
    "vec2 = vectorize(token_freq2)\n",
    "vec3= vectorize(token_freq3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1, 0, 0, 1, 0, 1, 1]"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "cos_sim12 = cos_similarity(vec1, vec2)\n",
    "cos_sim23 = cos_similarity(vec2, vec3)\n",
    "cos_sim31 = cos_similarity(vec3, vec1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'리바운드를 제압하는 자가 농구를 제압한다.' '화력으로 제압하는 부대가 상대를 제압한다.'의 유사도: 0.25\n",
      "'화력으로 제압하는 부대가 상대를 제압한다.' '리바운드를 잘하는 자가 농구에 필요하다.'의 유사도: 0.0\n",
      "'리바운드를 제압하는 자가 농구를 제압한다.' '리바운드를 잘하는 자가 농구에 필요하다.'의 유사도: 0.75\n"
     ]
    }
   ],
   "source": [
    "print(f\"'{sentence1}' '{sentence2}'의 유사도: {cos_sim12}\")\n",
    "print(f\"'{sentence2}' '{sentence3}'의 유사도: {cos_sim23}\")\n",
    "print(f\"'{sentence1}' '{sentence3}'의 유사도: {cos_sim31}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
