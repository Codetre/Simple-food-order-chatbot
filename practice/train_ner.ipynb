{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from config import resources as rsc\n",
    "from config import hyperparams as hparams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    \"\"\"데이터 파일을 파싱\n",
    "\n",
    "    파일은\n",
    "        1. 세미콜론으로 시작하는 원본 문장,\n",
    "        2. 그 다음엔 달러 기호로 시작하는 BIO 태깅된 문장,\n",
    "        3. 그 이후부터 다음 원본 문장 전까지는 각 토큰의 피처들의 나열(ID, token, POS, BIO)\n",
    "    이렇게 세 종류의 줄로 이루어져 있다.\n",
    "\n",
    "    :param file: BIO 토큰 정보를 가진 파일.\n",
    "    :return: 각 줄의 모든 피처를 열거한 리스트: [feats_sentence1, feats_sentence2, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # 어떤 라인인지 파악하기 위한 조건\n",
    "    def is_original_line(cur_i, lines):\n",
    "        if cur_i >= len(lines) - 1:\n",
    "            return False\n",
    "        else:\n",
    "            cur_line, next_line = lines[cur_i], lines[cur_i + 1]\n",
    "            cur_first_char = cur_line[0]\n",
    "            next_first_char = next_line[0]\n",
    "            return cur_first_char == \";\" and next_first_char == '$'\n",
    "\n",
    "    def is_ner_processed_line(cur_i, lines):\n",
    "        cur_line = lines[cur_i]\n",
    "        first_char = cur_line[0]\n",
    "        return first_char == \"$\"\n",
    "\n",
    "    def is_end_of_datapoint(line):\n",
    "        first_char = line[0]\n",
    "        return first_char == \"\\n\"\n",
    "\n",
    "    features_all_lines = []  # 파일 내 모든 문장의 토큰 피처들\n",
    "    with open(file, 'r', encoding=\"utf-8\") as fin:\n",
    "        lines = fin.readlines()\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            if is_original_line(i, lines):\n",
    "                features_a_line = []\n",
    "            elif is_ner_processed_line(i, lines):\n",
    "                continue\n",
    "            elif is_end_of_datapoint(line):\n",
    "                features_all_lines.append(features_a_line)\n",
    "            else:\n",
    "                features_a_line.append(line.split())\n",
    "    return features_all_lines\n",
    "\n",
    "\n",
    "def preprocess(corpus):\n",
    "    \"\"\"데이터 전처리 후 데이터셋 생성.\n",
    "\n",
    "    :param corpus: `read_file`의 리턴.\n",
    "    :return: dataset, maxlen, word_size, tag_size. 뒤 세 요소는 `build_model`에 쓰인다.\n",
    "        dataset: SliceDataset.\n",
    "            [0]: 인코딩된 고정 길이 토큰 시퀀스,\n",
    "            [1]: 원-핫 태그. 토큰 시퀀스가 어느 태그에 속하는지 의미.\n",
    "    \"\"\"\n",
    "\n",
    "    def divide_words_tags():\n",
    "        \"\"\"tokens, tags 따로 수집.\n",
    "\n",
    "        샘플별로 token과 BIO 태그를 분리한다.\n",
    "\n",
    "        :return: (tokens, bio_tags)\n",
    "        \"\"\"\n",
    "        words_all_sentences, tags_all_sentences = [], []\n",
    "        for info_about_a_sentence in corpus:\n",
    "            words_a_sentence, tags_a_sentence = [], []\n",
    "\n",
    "            for features_a_token in info_about_a_sentence:\n",
    "                word, tag = features_a_token[1], features_a_token[3]\n",
    "                words_a_sentence.append(word)\n",
    "                tags_a_sentence.append(tag)\n",
    "\n",
    "            words_all_sentences.append(words_a_sentence)\n",
    "            tags_all_sentences.append(tags_a_sentence)\n",
    "        return words_all_sentences, tags_all_sentences\n",
    "\n",
    "    def encode_sequences(sequences, **tokenizer_opts):\n",
    "        \"\"\"문자열 시퀀스를 정수형으로 인코딩.\n",
    "\n",
    "        :param sequences: words_all_sentences와 tags_all_sentences 두 가지 중 하나.\n",
    "        :param tokenizer_opts: 토크나이저 초기화 시 줄 옵션.\n",
    "            - words_all_sentences면 `oov_token=\"OOV\"`\n",
    "            - tags_all_sentences면 `lower=False` 부여.\n",
    "        :return: 토크나이저에 의해 인코딩된 시퀀스. 시퀀스 토큰 공간 크기, 최장 시퀀스 길이 세 가지.\n",
    "\n",
    "        \"\"\"\n",
    "        tokenizer = keras.preprocessing.text.Tokenizer(**tokenizer_opts)\n",
    "        tokenizer.fit_on_texts(sequences)\n",
    "        index_word = tokenizer.index_word\n",
    "        index_word[0] = 'PAD'\n",
    "        encoded_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "        vocab_size = len(index_word) + 1\n",
    "        maxlen = max(map(lambda sequence: len(sequence), encoded_sequences))\n",
    "\n",
    "        return encoded_sequences, vocab_size, maxlen\n",
    "\n",
    "    words_all_sentences, tags_all_sentences = divide_words_tags()\n",
    "    enc_word_sequences, word_size, maxlen = encode_sequences(\n",
    "        words_all_sentences, oov_token=\"OOV\")\n",
    "    padded_word_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "        enc_word_sequences, maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "    enc_tag_sequences, tag_size, _ = encode_sequences(\n",
    "        tags_all_sentences, lower=False)\n",
    "    padded_tag_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "        enc_tag_sequences, maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "    one_hot_tags = tf.keras.utils.to_categorical(\n",
    "        padded_tag_sequences, num_classes=tag_size)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (padded_word_sequences, one_hot_tags))\n",
    "\n",
    "    return dataset, maxlen, word_size, tag_size\n",
    "\n",
    "\n",
    "def build_model(input_len,\n",
    "                vocab_size: int,\n",
    "                tag_size: int):\n",
    "    \"\"\"NER 모델을 빌드한다.\n",
    "\n",
    "    입력 shape = (len_seq,). 정수형 인코딩된 토큰 시퀀스를 하나 받는다.\n",
    "    출력 shape = (tag_space_size,). 토큰 시퀀스가 각 BIO 태그일 확률을 나타낸다.\n",
    "\n",
    "    :param input_len: 입력 토큰 시퀀스의 길이. 입력된느 모든 시퀀스의 길이는 일정해야 한다.\n",
    "    :param vocab_size: 토큰 공간의 크기.\n",
    "    :param tag_size: BIO 태그 공간의 크기.\n",
    "    :return: 컴파일된 모델.\n",
    "    \"\"\"\n",
    "    input = keras.layers.Input(shape=[input_len, ])\n",
    "    embedding = keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=30,\n",
    "        mask_zero=True,  # 시퀀스를 0으로 패딩했다면 True로 값을 설정해 그 0이 패딩임을 고지.\n",
    "        input_length=input_len)(input)\n",
    "    bi_lstm = keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(\n",
    "            200,\n",
    "            return_sequences=True,\n",
    "            dropout=.5,\n",
    "            # T면 [timesteps, batch, feature] F면 [batch, timesteps, feature]의 입력\n",
    "            # 형상으로 True일 떄가 연산 효율은 더 좋다(기본값 False).\n",
    "            # time_major= True\n",
    "            recurrent_dropout=.25))(embedding)\n",
    "    dense = keras.layers.TimeDistributed(\n",
    "        keras.layers.Dense(tag_size, activation=\"softmax\"))(bi_lstm)\n",
    "\n",
    "    bi_lstm_model = keras.Model(inputs=input, outputs=dense)\n",
    "    bi_lstm_model.compile(loss=\"categorical_crossentropy\",\n",
    "                          optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                          metrics=[\"accuracy\"])\n",
    "\n",
    "    return bi_lstm_model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터 처리"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "corpus = read_file(rsc.train[\"ner\"])\n",
    "trainset, maxlen, word_size, tag_size = preprocess(corpus)\n",
    "trainset = trainset.batch(hparams.ner[\"batch_size\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 빌드"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "bi_lstm_model = build_model(input_len=maxlen,\n",
    "                            vocab_size=word_size,\n",
    "                            tag_size=tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 00:43:54.708377: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-20 00:43:54.714318: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13/485 [..............................] - ETA: 58:15:31 - loss: 2.2081 - accuracy: 0.7644"
     ]
    }
   ],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                                               patience=5,\n",
    "                                               mode=\"auto\")\n",
    "\n",
    "bi_lstm_history = bi_lstm_model.fit(trainset,\n",
    "                                    epochs=hparams.ner[\"epochs\"],\n",
    "                                    callbacks=[early_stopping])\n",
    "bi_lstm_model.save(rsc.model[\"ner\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 테스트 데이터로 성능 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def postprocess():\n",
    "    pass\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(rsc.model[\"ner\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sequences_to_tag(sequences, mapping_dict):\n",
    "    \"\"\"\"one-hot 결과를 읽을 수 있는 문자로 변환한다.\n",
    "\n",
    "    각 시퀀스는 토큰별 one-hot 추론 결과를 원소로 담고 있다. [num_seq, len_seq, len_one_hot]\n",
    "\n",
    "    :param sequences: 추론된 원-핫 형식 태그 콜렉션. 추론 함수의 결과 텐서를 기대한다.\n",
    "    :param mapping_dict: 인코딩된 태그값에 해당하는 원래 문자 태그의 매핑.\n",
    "    :return: `mapping_dict`에 의해 문자로 변환된 시퀀스들.\n",
    "    \"\"\"\n",
    "    tags_all_sequences = []\n",
    "    for sequence in sequences:\n",
    "        tags_a_sequence = []\n",
    "        for one_hot_score in sequence:\n",
    "            most_likely_index = np.argmax(one_hot_score)\n",
    "            pred_tag = mapping_dict[most_likely_index].replace(\"PAD\", \"O\")\n",
    "            tags_a_sequence.append(pred_tag)\n",
    "        tags_all_sequences.append(tags_a_sequence)\n",
    "\n",
    "    return tags_all_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 시간이 걸리므로 pickle 직렬화 보존하자.\n",
    "decoded_pred_tags = sequences_to_tag(pred_y, tag_index_word)\n",
    "decoded_test_tags = sequences_to_tag(test_y, tag_index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/corpus/korean_ner/decoded_pred_tags.pickle\", \"wb\") as fout:\n",
    "    pickle.dump(decoded_pred_tags, fout)\n",
    "\n",
    "with open(\"data/corpus/korean_ner/decoded_test_tags\", \"wb\") as fout:\n",
    "    pickle.dump(decoded_test_tags, fout)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report = classification_report(decoded_test_tags, decoded_pred_tags)\n",
    "f1 = f1_score(decoded_test_tags, decoded_pred_tags)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(report)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('tensorflow_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "72a30fb20ccb3b5f1323604f7dc65936ec8219365d815ccd0e4ed70e4e4b4b7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
